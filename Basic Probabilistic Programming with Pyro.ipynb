{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Probabilistic Programming with Pyro\n",
    "\n",
    "One of the interesting techniques which has become of interest lately is the use of probabilistic programming. \n",
    "If you've never heard of probabilistic programming before, you might conclude it is just programming with probabilities. That would be correct, but only part of the story. \n",
    "\n",
    "At its core, probabilistic programming is about finding underlying distributions which affect processes of interest. You define a process and how it works, and then allow your Probabilistic Programming Language (PPL) of choice to infer what are the likely values for factors which influence your process, and how likely they are.\n",
    "\n",
    "This is explained best with an example, and for this example I'll be using Pyro as the PPL of choice, as well as a simple case study largely inspired from the official Pyro tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Our Process - A Magical Kitchen Scale\n",
    "\n",
    "Imagine you had a kitchen scale, you put an object on it, and the scale tells you its weight. But imagine the scale is quite old, and is not very accurate. Every time you put the same object on it, it gives you a slightly different measurement. However, you observed that this scale, in some magical way errs in a way that forms a normal distribution around the true weight of the item, with a standard deviation of 0.1kg\n",
    "\n",
    "$$\\text{Observation} \\sim \\mathcal{N}(\\text{Weight}, 0.1)$$\n",
    "\n",
    "Let's describe this process (we'll be using PyTorch, which is what Pyro uses as its underlying tensor calculation engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def process(weight):\n",
    "    \n",
    "    my_dist = dist.Normal(weight, 0.1)\n",
    "    measurement = my_dist.sample()\n",
    "    return measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ask ourselves an easy question; \n",
    "\n",
    "> if we put a 0.5kg item on this scale, what are some possible values we will get back?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(process(0.5))\n",
    "print(process(0.5))\n",
    "print(process(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got back a collection of 3 values. Each time we ask our scale for an answer, we'll get a slightly different value. Sometimes the values will be very close to 0.5 (this will happen most of the time), and sometimes they will be very far. This was easy enough to implement though, nothing new in terms of programming there.\n",
    "\n",
    "Let's try a more difficult question; \n",
    "\n",
    "> What is the probability of observing a value above 0.63?\n",
    "\n",
    "We can answer that programmatically as well. We'll just generate many values, count all the values we generated, count all the values above 0.63, and find their ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "rough_estimate = np.sum([process(0.5) > 0.63 for i in range(1000)])/1000\n",
    "print(f'rough estimate: {rough_estimate}')\n",
    "\n",
    "reasonable_estimate = np.sum([process(0.5) > 0.63 for i in range(10000)])/10000\n",
    "print(f'reasonable estimate: {reasonable_estimate}')\n",
    "\n",
    "good_estimate = np.sum([process(0.5) > 0.63 for i in range(100000)])/100000\n",
    "print(f'good estimate: {good_estimate}')\n",
    "\n",
    "true_estimate = 1.0 - norm(0.5, 0.1).cdf(0.63)\n",
    "print(f'true estimate: {true_estimate}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the process enough times, we'll get an estimate that is good for all intents and purposes. This took a bit more work, and some more time, but overall this workflow wasn't too hard either.\n",
    "\n",
    "Let's try an even more difficult question; \n",
    "\n",
    "> I've observed a value of 0.63kg, what is the likeliest true weight of the item?\n",
    "\n",
    "This might take you by surprise, but after a bit of thinking, you might recall my magical inaccurate-yet-well-behaved-scale reports a normal distribution around the true weight of the item, so with no further information, the most likely weight is in fact 0.63kg. \n",
    "\n",
    "However, this was largely easy because the distribution is normal around the true weight of the scale. What if it wasn't normal? what if it wasn't even something pre-built into Python?\n",
    "\n",
    "Now let's try a hard question that will motivate our use of probabilistic programming?\n",
    "\n",
    "> Let's imagine we got the following set of measurements, or observations, from our scale: 0.74kg, 0.98kg, 0.66kg, 0.75kg, 0.84kg, and 0.74kg, what is the likeliest true weight of the object?\n",
    "\n",
    "**The true weight I used to generate those values is 0.8kg, but let's see how we might find that out**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our process in Pyro\n",
    "\n",
    "The question just in the last section is exactly the type of question probabilistic programming is designed to answer. The general form is \"given some observations, and a good understanding of the process, can I infer something about the hidden values in my process?\"\n",
    "\n",
    "Now we would like to know, what is the likeliest weight of our item. We'll now employ Pyro to answer this question.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Pyro only works on torch tensors\n",
    "observations = torch.tensor([0.74, 0.98, 0.66, 0.75, 0.84, 0.74])\n",
    "\n",
    "print(f'The mean of the observations is: {torch.mean(observations)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define our process again, but with a few changes. We'll also rename the function from `process` to `model` since that is usually how Pyro code is written:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our libraries\n",
    "import pyro\n",
    "import pyro.distributions as pyrodist\n",
    "\n",
    "pyro.set_rng_seed(0)\n",
    "\n",
    "# Define the process\n",
    "def model(observations):\n",
    "    \n",
    "    # 1. Let's define a prior distribution on the likely values of our weight.\n",
    "    weight_prior = pyrodist.Normal(0.785, 1.0)\n",
    "    \n",
    "    # 2. Sample a value from the weight distributoin\n",
    "    weight = pyro.sample(\"weight1\", weight_prior)\n",
    "    \n",
    "    # 3. Now use a that value to define our scale (remember our scale gives us values\n",
    "    # from Normal(weight, 0.1))\n",
    "    my_dist = pyrodist.Normal(weight, 0.1)\n",
    "    \n",
    "    \n",
    "    # 4. For each of the observations, let's draw a sample from our distribution.\n",
    "    # HOWEVER, this is a conditioned sample, it's a sample conditioned on\n",
    "    # the observations we have\n",
    "    \n",
    "    for i,observation in enumerate(observations):\n",
    "        measurement = pyro.sample(f'obs_{i}', my_dist, obs=observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might seem like a lot, so let's unpack how that function works.\n",
    "\n",
    "We are trying to find a distribution over possible values of the weight, given the observations we have.\n",
    "\n",
    "We first define a prior distribution on those values, our best guess for what the probability distribution might be, at (1).\n",
    "\n",
    "Then, we sample from that distribution at (2). However, this is now named distribution (we named it `\"weight1\"`). This is our way of telling Pyro to optimize the distribution from which the weight came from. Behind the scenes, Pyro will modify this distribution to be more in-line with our observations.\n",
    "\n",
    "At (3), we define the way the scale works. Recall that to produce measurements, the scale returns values from a normal distribution centered at the weight, and with a standard deviation of 0.1.\n",
    "\n",
    "Finally, at (4), we sample from the distribution we defined at (3). Notice these are also named samples, but they are conditioned on the observations. This is where we tell Pyro how our value for \"weight1\" relates to the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring\n",
    "\n",
    "Now we are going to use our model to find a suitable estimate for the weight of the item that gave us our measurements. We're going to use a family of algorithms called Markov Chain Monte Carlo (MCMC), and a particular instance called Hamiltonian Monte Carlo (HMC).\n",
    "\n",
    "In essence, this algorithm will build a probability distribution of our named distribution \"weight1\" based on the observations we defined that distribution affects (the variables we named \"obs_{i}\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import MCMC, HMC\n",
    "\n",
    "# 1. Clear storage of named parameters\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# 2. Define the MCMC kernel function we will employ, and tell\n",
    "# it to use the model function we defined as the basis for\n",
    "# sampling\n",
    "my_kernel = HMC(model)\n",
    "\n",
    "\n",
    "# 3. Define the MCMC algorithm with our specific\n",
    "# implementation of choice and the number of samples\n",
    "# to use to evaluate the most likely distribution\n",
    "# of \"weight1\".\n",
    "my_mcmc = MCMC(my_kernel,\n",
    "               num_samples=20000,\n",
    "               warmup_steps=100)\n",
    "\n",
    "# 4. Run the algorithm, send our observations \n",
    "# (notice this is the parameter model(observations) recieves)\n",
    "my_mcmc.run(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack what happened here as well;\n",
    "\n",
    "At (1) we clear all named samples from our Pyro storage. Pyro uses a special, built-in, dictionary like object to keep track of its estimates for items we request. It's important to clear that so if we run this code several times, previous runs won't skew our results.\n",
    "\n",
    "At (2) and (3) we define an MCMC algorithm which uses an HMC implementation to estimate the distribution of the named variable \"weight1\". The MCMC algorithm runs for a certain number of iterations, in this case 20,000 iterations.\n",
    "However, the first few estimates it draws are very sensitive to initial random conditions, so we tend to discard these estimates. All remaining 20,000 samples will be used to build our posterior estimate of \"weight1\"\n",
    "\n",
    "Finally at (4) we run our MCMC algorithm. Notice we have to send our observations into the model (as we defined it before to be accepting a series of observations). This is where we send all variables the `model` function accepts into it.\n",
    "\n",
    "Now, let's draw all the samples we got when estimating \"weight1\", convert those samples to a numpy array (they come back as a pytorch tensor), and plot them as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .get_samples() returns a dictionary\n",
    "my_mcmc.get_samples()['weight1'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.distplot(my_mcmc.get_samples()['weight1'].numpy(), kde=False, label=\"weight1\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Weight (kg)\")\n",
    "plt.ylabel(\"Observed Samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the summary and see what is the median value of this distribution to find the most likely value, as well as a 90% credibility interval. So we see the median value is about 0.79kg, and we're 90% confident the value for the weight of the item is between 0.72kg - 0.85kg (this is also room for us to judge if we're happy with that 90% range). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mcmc.summary(prob=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might now say \"Well, you had a pretty good prior, your mean was very close to the true value of 0.8kg. What if you had used a less informative prior?\"\n",
    "\n",
    "Let's explore that scenario as well. This time we'll use a uniform distribution between 0.0 (the weight can't be negative), and 2kg (seems like a good upper threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(observations):\n",
    "    \n",
    "    # 1. Let's define a prior distribution on the likely values of our weight.\n",
    "    weight_prior = pyrodist.Uniform(0.0, 2.0)\n",
    "    \n",
    "    # 2. Sample a value from the weight distributoin\n",
    "    weight = pyro.sample(\"weight2\", weight_prior)\n",
    "    \n",
    "    # 3. Now use a that value to define our scale (remember our scale gives us values\n",
    "    # from Normal(weight, 0.1))\n",
    "    my_dist = pyrodist.Normal(weight, 0.1)\n",
    "    \n",
    "    \n",
    "    # 4. For each of the observations, let's draw a sample from our distribution.\n",
    "    # HOWEVER, this is a conditioned sample, it's a sample conditioned on\n",
    "    # the observations we have\n",
    "    for i,observation in enumerate(observations):\n",
    "        measurement = pyro.sample(f'obs_{i}', my_dist, obs=observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kernel = HMC(model)\n",
    "\n",
    "my_mcmc2 = MCMC(my_kernel,\n",
    "                num_samples=20000,\n",
    "                warmup_steps=100)\n",
    "\n",
    "my_mcmc2.run(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mcmc2.summary(prob=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having this less informative prior did not make much of a difference. The algorithm is 90% confident the weight lies between 0.72kg and 0.85kg, and the mean of those values is again centered at 0.79kg (very close 0.8kg which was used).\n",
    "\n",
    "We can grab the samples for this new approximation again, convert them to a numpy array again, and plot the results as a distribution. You will see the distributions overlap very strongly, even though two very different priors were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mcmc2.get_samples()['weight2'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.distplot(my_mcmc.get_samples()['weight1'].numpy(), kde=False, label=\"weight1\")\n",
    "sns.distplot(my_mcmc2.get_samples()['weight2'].numpy(), kde=False, label=\"weight2\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Weight (kg)\")\n",
    "plt.ylabel(\"Observed Samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating Our Weight using Stochastic Variational Inference\n",
    "\n",
    "You may notice that drawing all the samples using an MCMC method takes quite a bit of time. Pyro offers an alternative method of evaluating our hidden parameters that is faster, and can provide equally good results for some problems: Stochastic Variational Inference (SVI).\n",
    "\n",
    "SVI methods treat this inference problem as an optimization one. Under the SVI method, we choose another distribution which is defined by some parameters (for example, the normal distribution is defined by its mean and standard deviation), and we optimize those parameters until our chosen distribution provides results which are \"close enough\" to our observations. \n",
    "\n",
    "This alternative method of evaluating a posterior requires an additional function, which is called a guide. This function defines our parameters, as well as our proposed distribution to match the observations.\n",
    "\n",
    "Let's define our model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(observations):\n",
    "    \n",
    "    weight_prior = pyrodist.Uniform(0.0, 2.0)\n",
    "    \n",
    "    weight = pyro.sample(\"weight3\", weight_prior)\n",
    "    \n",
    "    my_dist = pyrodist.Normal(weight, 0.1)\n",
    "    \n",
    "    \n",
    "    for i,observation in enumerate(observations):\n",
    "        measurement = pyro.sample(f'obs_{i}', my_dist, obs=observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's define our guide function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import constraints\n",
    "\n",
    "def guide(observations):\n",
    "    # 1. The guide must take exactly the same parameters as the model\n",
    "    # even if it does not use them\n",
    "    \n",
    "    # 2. Our intial guesses for the mean and standard deviation\n",
    "    # Let's guess that the mean is 0.0, and the standard deviation\n",
    "    # is 1.0\n",
    "    mean_parameter = pyro.param(\"mu\", torch.tensor(0.0))\n",
    "    std_parameter = pyro.param(\"sigma\", torch.tensor(1.0), constraint=constraints.positive)\n",
    "    \n",
    "    # 3. Define the weight distribution these parameters define\n",
    "    # We choose a normal distribution\n",
    "    weight_distribution = pyrodist.Normal(mean_parameter, std_parameter)\n",
    "    \n",
    "    # 4. Every pyro.sample statement in the model\n",
    "    # must have a matching pyro.sample statement\n",
    "    # in the guide\n",
    "    weight = pyro.sample(\"weight3\", weight_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the guide and the model functions must accept the same inputs, even if the guide doesn't use any of them at all.\n",
    "\n",
    "At (2) and (3) we say take a normal distribution, and adjust its parameters (for a normal distribution it is just the mean and standard deviation) so that it is more likely to produce the observed observation. In (2) we have to provide initial guesses for those parameters (notice the `pyro.param` statement, which is different than the `pyro.sample` in that it registers a parameter, not a distribution).\n",
    "\n",
    "At (4) we draw a weight from the distribution we defined in (3). We don't actually have to store that value into a python variable, we only need the `pyro.sample` statement which is named exactly the same as it is in the model function.\n",
    "\n",
    "Now this becomes an optimization problem, we simply need to optimize `\"mu\"` and `\"sigma\"` such that the normal distribution defined by them is closer to producing our observed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import more fun items\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import ClippedAdam\n",
    "\n",
    "# Clear all the parameter store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# Define our SVI algorithm. It accepts our model\n",
    "# our guide, an optimizer, and a loss function\n",
    "# to minimize\n",
    "my_svi = SVI(model=model,\n",
    "             guide=guide,\n",
    "             optim=ClippedAdam({\"lr\": 0.001}),\n",
    "             loss=Trace_ELBO())\n",
    "\n",
    "# Decide on the number of optimization steps to \n",
    "# take, and run the optimizer for that\n",
    "# many steps.\n",
    "\n",
    "# We sent the parameters into model and guide\n",
    "# in the .step() function\n",
    "optimization_steps = 10000\n",
    "for i in range(optimization_steps):\n",
    "    \n",
    "    loss = my_svi.step(observations)\n",
    "    \n",
    "    if (i % 100 == 0):\n",
    "        print(f'iter: {i}, loss: {round(loss,2)}', end=\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing the optimization, we can ask pyro for the found values of `\"mu\"` and `\"sigma\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\"mu\" is {pyro.param(\"mu\")}')\n",
    "print(f'\"sigma\" is {pyro.param(\"sigma\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask for the same statistics as before about \"weight3\", such as the 90% credibility interval, but we have to do a bit more work for this.\n",
    "\n",
    "We need to use our found values to generate new samples (MCMC works by generating samples, so we don't need that extra step after the algorithm runs then):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "sample_num = 20000\n",
    "predictive = Predictive(model=model, \n",
    "                        guide=guide,\n",
    "                        num_samples=sample_num,\n",
    "                        return_sites=(\"weight3\",))\n",
    "\n",
    "samples = predictive(observations)\n",
    "\n",
    "\n",
    "stats = {}\n",
    "for key, values in samples.items():\n",
    "\n",
    "    stats = {\"weight_mean\": torch.mean(values, axis=0).detach().numpy(),\n",
    "             \"weight_5%\": values.kthvalue(int(sample_num*0.05), axis=0)[0].detach().numpy(),\n",
    "             \"weight_95%\": values.kthvalue(int(sample_num*0.95), axis=0)[0].detach().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.distplot(my_mcmc.get_samples()['weight1'].numpy(), kde=False, label=\"weight1-MCMC\", color=\"red\")\n",
    "sns.distplot(my_mcmc2.get_samples()['weight2'].numpy(), kde=False, label=\"weight2-MCMC\", color=\"blue\")\n",
    "sns.distplot(samples['weight3'].detach().numpy(), kde=False, label=\"weight3-SVI\", color=\"orange\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Weight (kg)\")\n",
    "plt.ylabel(\"Observed Samples\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
