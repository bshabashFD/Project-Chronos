{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start with a single gamma distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max = 200\n",
    "DATA_POINT_NUMBER = int(1*1e7)\n",
    "\n",
    "\n",
    "shape = 3.0\n",
    "scale = 20.0\n",
    "\n",
    "print(f\"Will produce {DATA_POINT_NUMBER} data points for each curve\")\n",
    "\n",
    "x_range = np.linspace(start=1/np.sqrt(np.finfo(np.float32).max), stop=x_max, num=DATA_POINT_NUMBER)\n",
    "#x_range = x_range.reshape(x_range.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shape)\n",
    "print(scale)\n",
    "print(x_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_range = gamma(a=shape, scale=scale).cdf(x=x_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_range, y_range);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the histogram\n",
    "plt.hist(y_range, bins=200, log=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_data = np.array([shape] * x_range.shape[0])\n",
    "scale_data = np.array([scale] * x_range.shape[0])\n",
    "\n",
    "x_data = np.array([shape_data, scale_data, x_range]).T\n",
    "\n",
    "print(x_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(x_data).sum() + np.isinf(x_data).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(y_range).sum() + np.isinf(y_range).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pytorch = torch.tensor(x_data, dtype=torch.float32)\n",
    "x_pytorch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(x_pytorch).sum() + np.isinf(x_pytorch).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pytorch = torch.tensor(y_range, dtype=torch.float32).view(y_range.shape[0], -1)\n",
    "y_pytorch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(y_pytorch).sum() + np.isinf(y_pytorch).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [16, 8]\n",
    "\n",
    "layers.insert(0, 3)\n",
    "print(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = []\n",
    "for i in range(len(layers)-1):\n",
    "    input_size = layers[i]\n",
    "    output_size = layers[i+1]\n",
    "    \n",
    "    layer_list.append(torch.nn.Linear(input_size, output_size))\n",
    "    layer_list.append(torch.nn.LeakyReLU())\n",
    "    #layer_list.append(torch.nn.BatchNorm1d(output_size))\n",
    "\n",
    "layer_list.append(torch.nn.Linear(output_size, 1))\n",
    "layer_list.append(torch.nn.Sigmoid())\n",
    "for layer in layer_list:\n",
    "    print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 linear layers originally\n",
    "net = torch.nn.Sequential(*layer_list)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_list = []\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "EPOCH = 6\n",
    "\n",
    "\n",
    "# start training\n",
    "while epoch < EPOCH:\n",
    "    BATCH_SIZE = 128 #np.random.randint(low=32, high=512)\n",
    "    \n",
    "    torch_dataset = Data.TensorDataset(x_pytorch, y_pytorch)\n",
    "\n",
    "    loader = Data.DataLoader(\n",
    "        dataset=torch_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, num_workers=2,)\n",
    "    \n",
    "    \n",
    "    epoch_loss_list = []\n",
    "    for step, (batch_x, batch_y) in enumerate(loader): # for each training step\n",
    "        \n",
    "        try:\n",
    "            prediction = net(batch_x)     # input x and predict based on x\n",
    "\n",
    "            loss = loss_func(prediction, batch_y)     # must be (1. nn output, 2. target)\n",
    "            if ((step % 500) == 0):\n",
    "                current_pct = round(100*step/(x_pytorch.shape[0]//BATCH_SIZE),2)\n",
    "                print(f\"Epoch: {epoch}\", \n",
    "                      f\"progress: {current_pct}%\", \n",
    "                      f\"BATCH: {BATCH_SIZE}\",\n",
    "                      f\"Loss: {round(np.mean(epoch_loss_list), 5)}\", \n",
    "                      end=\"\\r\")\n",
    "                time.sleep(0.1)\n",
    "\n",
    "            epoch_loss_list.append(np.sqrt(loss.item()))\n",
    "\n",
    "            optimizer.zero_grad()   # clear gradients for next train\n",
    "            loss.backward()         # backpropagation, compute gradients\n",
    "            optimizer.step()        # apply gradients\n",
    "            \n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "    losses_list.append(np.mean(epoch_loss_list))\n",
    "    epoch += 1\n",
    "    torch.save(net.state_dict(), \"my_model\")\n",
    "    \n",
    "print(f\"Epoch: {epoch}\", \n",
    "      f\"progress: {current_pct}%\", \n",
    "      f\"BATCH: {BATCH_SIZE}\",\n",
    "      f\"Loss: {round(np.mean(epoch_loss_list), 5)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_list);\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = []\n",
    "scales = []\n",
    "rmse_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(100):\n",
    "    new_shape = 3.0\n",
    "    new_scale = 20.0\n",
    "    x = []\n",
    "    y = []\n",
    "    y_true = []\n",
    "\n",
    "    my_gamma = gamma(a=new_shape, scale=new_scale)\n",
    "    x_range = np.arange(0.0, 500.0, 0.1) \n",
    "    y_true = my_gamma.cdf(x=x_range)\n",
    "    \n",
    "    \n",
    "    new_shape_range = [new_shape] * x_range.shape[0]\n",
    "    new_scale_range = [new_scale] * x_range.shape[0]\n",
    "    \n",
    "    x_input = torch.tensor(np.array([new_shape_range, new_scale_range, x_range], dtype=np.float32).T)\n",
    "    \n",
    "    net.eval()\n",
    "    y_pred = net(x_input).squeeze()\n",
    "    \n",
    "    \n",
    "    y_pred = np.array(y_pred.detach())\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "\n",
    "    my_rmse = np.sqrt(np.nanmean((y_true - y_pred)**2))\n",
    "\n",
    "\n",
    "    scales.append(new_scale)\n",
    "    shapes.append(new_shape)\n",
    "    rmse_scores.append(my_rmse)\n",
    "    print(j, end=\"\\r\")\n",
    "\n",
    "plt.plot(x_range,y_pred, label=\"Prediction\")\n",
    "plt.plot(x_range,y_true, label=\"True CDF\")\n",
    "plt.title(f\"shape={new_shape}, scale={new_scale}, RMSE={my_rmse}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(scales, shapes, c=rmse_scores)\n",
    "plt.xlabel(\"Scale\")\n",
    "plt.ylabel(\"Shape\")\n",
    "print(np.nanmin(rmse_scores), \"-\",np.nanmax(rmse_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_max = 20.0\n",
    "scale_max = 20.0\n",
    "x_max = 1000\n",
    "DATA_POINT_NUMBER = int(5*1e7)\n",
    "\n",
    "\n",
    "shape_range = np.random.random(size=100) * shape_max\n",
    "scale_range1 = np.random.random(size=100) * scale_max \n",
    "scale_range2 = np.random.random(size=100) * 0.01\n",
    "\n",
    "scale_range = np.concatenate([scale_range1,scale_range2])\n",
    "\n",
    "data_points_per_curve = int(DATA_POINT_NUMBER/(shape_range.shape[0]*scale_range.shape[0]))\n",
    "print(f\"Will produce {data_points_per_curve} data points for each curve\")\n",
    "\n",
    "x_range = np.linspace(start=1/np.sqrt(np.finfo(np.float32).max), stop=x_max, num=data_points_per_curve)\n",
    "#x_range = x_range.reshape(x_range.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shape_max)\n",
    "print(scale_max)\n",
    "print(x_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_data_np = np.zeros(shape=(DATA_POINT_NUMBER, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_marker = 0\n",
    "current_shape_marker = 0\n",
    "current_scale_marker = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "while current_marker < DATA_POINT_NUMBER:\n",
    "    \n",
    "    current_shape_marker = 0\n",
    "    while current_shape_marker < shape_range.shape[0]:\n",
    "        new_shape = shape_range[current_shape_marker]\n",
    "        current_scale_marker = 0 \n",
    "        \n",
    "        \n",
    "        while current_scale_marker < scale_range.shape[0]:\n",
    "            new_scale = scale_range[current_scale_marker]\n",
    "            \n",
    "            \n",
    "            my_gamma = gamma(a=new_shape, scale=new_scale)\n",
    "            \n",
    "            \n",
    "            new_pdfs = my_gamma.pdf(x=x_range)\n",
    "            new_ys = my_gamma.cdf(x=x_range)\n",
    "            \n",
    "            \n",
    "            gamma_data_np[current_marker:current_marker+x_range.shape[0], 0] = new_shape\n",
    "            gamma_data_np[current_marker:current_marker+x_range.shape[0], 1] = new_scale\n",
    "            gamma_data_np[current_marker:current_marker+x_range.shape[0], 2] = new_pdfs\n",
    "            \n",
    "            gamma_data_np[current_marker:current_marker+x_range.shape[0], 3] = x_range[:]\n",
    "            gamma_data_np[current_marker:current_marker+x_range.shape[0], 4] = new_ys[:]\n",
    "            \n",
    "            current_marker += x_range.shape[0]\n",
    "            if ((current_marker % 10) == 0):\n",
    "                print(f\"shape: {current_shape_marker}\",\n",
    "                      f\", scale: {current_scale_marker}\",\n",
    "                      f\", progress: {round(100*current_marker/DATA_POINT_NUMBER,13)}%\", end=\"\\r\")\n",
    "                \n",
    "            current_scale_marker += 1\n",
    "            \n",
    "        current_shape_marker += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the histogram\n",
    "plt.hist(gamma_data_np[:, -1], bins=200, log=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savetxt('gamma_numpy_data.csv', gamma_data_np, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pytorch = torch.tensor(gamma_data_np[:current_marker, :-1], dtype=torch.float32)\n",
    "x_pytorch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pytorch = torch.tensor(gamma_data_np[:current_marker, -1], dtype=torch.float32).view(current_marker, -1)\n",
    "y_pytorch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [64, 32, 32, 32, 16]\n",
    "\n",
    "layers.insert(0, x_pytorch.shape[1])\n",
    "print(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = []\n",
    "for i in range(len(layers)-1):\n",
    "    input_size = layers[i]\n",
    "    output_size = layers[i+1]\n",
    "    \n",
    "    layer_list.append(torch.nn.Linear(input_size, output_size))\n",
    "    layer_list.append(torch.nn.PReLU())\n",
    "    #layer_list.append(torch.nn.Dropout(p=0.1))\n",
    "    #layer_list.append(torch.nn.BatchNorm1d(output_size))\n",
    "\n",
    "    \n",
    "layer_list.append(torch.nn.Linear(output_size, 2))\n",
    "layer_list.append(torch.nn.Sigmoid())\n",
    "\n",
    "layer_list.append(torch.nn.Linear(2, 1))\n",
    "#layer_list.append(torch.nn.Sigmoid())\n",
    "for layer in layer_list:\n",
    "    print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 linear layers originally\n",
    "net = torch.nn.Sequential(*layer_list)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.L1Loss()  # this is for regression mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_list = []\n",
    "max_loss_list = []\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "EPOCH = 60\n",
    "\n",
    "\n",
    "# start training\n",
    "while epoch < EPOCH:\n",
    "    BATCH_SIZE = np.random.randint(low=32, high=512)\n",
    "    #BATCH_SIZE = 128\n",
    "    \n",
    "    torch_dataset = Data.TensorDataset(x_pytorch, y_pytorch)\n",
    "\n",
    "    loader = Data.DataLoader(\n",
    "        dataset=torch_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, num_workers=2,)\n",
    "    \n",
    "    \n",
    "    epoch_loss_list = []\n",
    "    for step, (batch_x, batch_y) in enumerate(loader): # for each training step\n",
    "        \n",
    "        try:\n",
    "            prediction = net(batch_x)     # input x and predict based on x\n",
    "\n",
    "            loss = loss_func(prediction, batch_y)     # must be (1. nn output, 2. target)\n",
    "            if ((step % 500) == 0):\n",
    "                current_pct = round(100*step/(x_pytorch.shape[0]//BATCH_SIZE),2)\n",
    "                print(f\"Epoch: {epoch}\", \n",
    "                      f\"progress: {current_pct}%\", \n",
    "                      f\"BATCH: {BATCH_SIZE}\",\n",
    "                      f\"Loss: {round(np.mean(epoch_loss_list), 5)}\", \n",
    "                      f\"Max Loss: {round(np.max(epoch_loss_list), 5)}\",\n",
    "                      end=\"\\r\")\n",
    "                time.sleep(0.1)\n",
    "\n",
    "            #epoch_loss_list.append(np.sqrt(loss.item()))\n",
    "            epoch_loss_list.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()   # clear gradients for next train\n",
    "            loss.backward()         # backpropagation, compute gradients\n",
    "            optimizer.step()        # apply gradients\n",
    "            \n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "    losses_list.append(np.mean(epoch_loss_list))\n",
    "    max_loss_list.append(np.max(epoch_loss_list))\n",
    "    epoch += 1\n",
    "    torch.save(net.state_dict(), \"my_model\")\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_list, label=\"Mean Loss\")\n",
    "plt.plot(max_loss_list, label=\"Max Loss\")\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_list, label=\"Mean Loss\")\n",
    "plt.plot(max_loss_list, label=\"Max Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = []\n",
    "scales = []\n",
    "rmse_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_x = None\n",
    "worst_y_true = None\n",
    "worst_y_pred = None\n",
    "worst_rmse = 0\n",
    "worst_shape, worst_scale = None, None\n",
    "worst_y_pdf = None\n",
    "\n",
    "for j in range(10000):\n",
    "    new_shape = np.random.random()*40.0\n",
    "    new_scale = np.random.random()*40.0\n",
    "    x = []\n",
    "    y = []\n",
    "    y_true = []\n",
    "\n",
    "    my_gamma = gamma(a=new_shape, scale=new_scale)    \n",
    "    x_range = np.arange(0.0, 500.0, 0.1) \n",
    "    pdf_range = my_gamma.pdf(x=x_range)\n",
    "    \n",
    "    y_true = my_gamma.cdf(x=x_range)\n",
    "    \n",
    "    \n",
    "    new_shape_range = [new_shape] * x_range.shape[0]\n",
    "    new_scale_range = [new_scale] * x_range.shape[0]\n",
    "    \n",
    "    x_input = torch.tensor(np.array([new_shape_range, new_scale_range, pdf_range, x_range], dtype=np.float32).T)\n",
    "    \n",
    "    net.eval()\n",
    "    y_pred = net(x_input).squeeze()\n",
    "    \n",
    "    \n",
    "    y_pred = np.array(y_pred.detach())\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "\n",
    "    my_rmse = np.nanmax(np.abs((y_true - y_pred)))\n",
    "    if (my_rmse > worst_rmse):\n",
    "        worst_x = x_range\n",
    "        worst_y_true = y_true\n",
    "        worst_y_pred = y_pred\n",
    "        worst_rmse = my_rmse\n",
    "        worst_shape, worst_scale = new_shape, new_scale\n",
    "        worst_y_pdf = pdf_range\n",
    "\n",
    "\n",
    "    scales.append(new_scale)\n",
    "    shapes.append(new_shape)\n",
    "    rmse_scores.append(my_rmse)\n",
    "    print(j, end=\"\\r\")\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(worst_x, worst_y_pred, label=\"Prediction\")\n",
    "plt.plot(worst_x, worst_y_true, label=\"True CDF\")\n",
    "plt.plot(worst_x, worst_y_pdf, label=\"True PDF\")\n",
    "plt.title(f\"shape={worst_shape}, scale={worst_scale}, RMSE={worst_rmse}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(scales, shapes, c=-np.array(rmse_scores), s=30, cmap = 'RdBu')\n",
    "plt.xlabel(\"Scale\")\n",
    "plt.ylabel(\"Shape\")\n",
    "print(np.nanmin(rmse_scores), \"-\",np.nanmax(rmse_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(worst_x, np.log(worst_y_true), label=\"True CDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will produce 500000 data points for each curve\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a_max = 20.0\n",
    "x_max = 1000\n",
    "DATA_POINT_NUMBER = int(5*1e7)\n",
    "\n",
    "\n",
    "a_range = np.random.random(size=100) * a_max\n",
    "\n",
    "data_points_per_curve = int(DATA_POINT_NUMBER/a_range.shape[0])\n",
    "print(f\"Will produce {data_points_per_curve} data points for each curve\")\n",
    "\n",
    "x_range = np.random.random(size=data_points_per_curve) * x_max \n",
    "#x_range = x_range.reshape(x_range.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(a_max)\n",
    "print(x_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_data_np = np.zeros(shape=(DATA_POINT_NUMBER, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_marker = 0\n",
    "current_a_marker = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "while current_marker < DATA_POINT_NUMBER:\n",
    "    \n",
    "    current_a_marker = 0\n",
    "    while current_shape_marker < shape_range.shape[0]:\n",
    "        new_a = a_range[current_a_marker]\n",
    "\n",
    "        my_lower_gamma\n",
    "            \n",
    "            new_pdfs = my_gamma.pdf(x=x_range)\n",
    "            new_ys = my_gamma.cdf(x=x_range)\n",
    "            \n",
    "            \n",
    "            gamma_data_np[current_marker:current_marker+x_range.shape[0], 0] = new_shape\n",
    "            gamma_data_np[current_marker:current_marker+x_range.shape[0], 1] = new_scale\n",
    "            gamma_data_np[current_marker:current_marker+x_range.shape[0], 2] = new_pdfs\n",
    "            \n",
    "            gamma_data_np[current_marker:current_marker+x_range.shape[0], 3] = x_range[:]\n",
    "            gamma_data_np[current_marker:current_marker+x_range.shape[0], 4] = new_ys[:]\n",
    "            \n",
    "            current_marker += x_range.shape[0]\n",
    "            if ((current_marker % 10) == 0):\n",
    "                print(f\"shape: {current_shape_marker}\",\n",
    "                      f\", scale: {current_scale_marker}\",\n",
    "                      f\", progress: {round(100*current_marker/DATA_POINT_NUMBER,13)}%\", end=\"\\r\")\n",
    "                \n",
    "            current_scale_marker += 1\n",
    "            \n",
    "        current_shape_marker += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the histogram\n",
    "plt.hist(gamma_data_np[:, -1], bins=200, log=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savetxt('gamma_numpy_data.csv', gamma_data_np, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pytorch = torch.tensor(gamma_data_np[:current_marker, :-1], dtype=torch.float32)\n",
    "x_pytorch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pytorch = torch.tensor(gamma_data_np[:current_marker, -1], dtype=torch.float32).view(current_marker, -1)\n",
    "y_pytorch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [64, 32, 32, 32, 16]\n",
    "\n",
    "layers.insert(0, x_pytorch.shape[1])\n",
    "print(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = []\n",
    "for i in range(len(layers)-1):\n",
    "    input_size = layers[i]\n",
    "    output_size = layers[i+1]\n",
    "    \n",
    "    layer_list.append(torch.nn.Linear(input_size, output_size))\n",
    "    layer_list.append(torch.nn.PReLU())\n",
    "    #layer_list.append(torch.nn.Dropout(p=0.1))\n",
    "    #layer_list.append(torch.nn.BatchNorm1d(output_size))\n",
    "\n",
    "    \n",
    "layer_list.append(torch.nn.Linear(output_size, 2))\n",
    "layer_list.append(torch.nn.Sigmoid())\n",
    "\n",
    "layer_list.append(torch.nn.Linear(2, 1))\n",
    "#layer_list.append(torch.nn.Sigmoid())\n",
    "for layer in layer_list:\n",
    "    print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 linear layers originally\n",
    "net = torch.nn.Sequential(*layer_list)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_func = torch.nn.L1Loss()  # this is for regression mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_list = []\n",
    "max_loss_list = []\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "EPOCH = 60\n",
    "\n",
    "\n",
    "# start training\n",
    "while epoch < EPOCH:\n",
    "    BATCH_SIZE = np.random.randint(low=32, high=512)\n",
    "    #BATCH_SIZE = 128\n",
    "    \n",
    "    torch_dataset = Data.TensorDataset(x_pytorch, y_pytorch)\n",
    "\n",
    "    loader = Data.DataLoader(\n",
    "        dataset=torch_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, num_workers=2,)\n",
    "    \n",
    "    \n",
    "    epoch_loss_list = []\n",
    "    for step, (batch_x, batch_y) in enumerate(loader): # for each training step\n",
    "        \n",
    "        try:\n",
    "            prediction = net(batch_x)     # input x and predict based on x\n",
    "\n",
    "            loss = loss_func(prediction, batch_y)     # must be (1. nn output, 2. target)\n",
    "            if ((step % 500) == 0):\n",
    "                current_pct = round(100*step/(x_pytorch.shape[0]//BATCH_SIZE),2)\n",
    "                print(f\"Epoch: {epoch}\", \n",
    "                      f\"progress: {current_pct}%\", \n",
    "                      f\"BATCH: {BATCH_SIZE}\",\n",
    "                      f\"Loss: {round(np.mean(epoch_loss_list), 5)}\", \n",
    "                      f\"Max Loss: {round(np.max(epoch_loss_list), 5)}\",\n",
    "                      end=\"\\r\")\n",
    "                time.sleep(0.1)\n",
    "\n",
    "            #epoch_loss_list.append(np.sqrt(loss.item()))\n",
    "            epoch_loss_list.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()   # clear gradients for next train\n",
    "            loss.backward()         # backpropagation, compute gradients\n",
    "            optimizer.step()        # apply gradients\n",
    "            \n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "    losses_list.append(np.mean(epoch_loss_list))\n",
    "    max_loss_list.append(np.max(epoch_loss_list))\n",
    "    epoch += 1\n",
    "    torch.save(net.state_dict(), \"my_model\")\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_list, label=\"Mean Loss\")\n",
    "plt.plot(max_loss_list, label=\"Max Loss\")\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_list, label=\"Mean Loss\")\n",
    "plt.plot(max_loss_list, label=\"Max Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = []\n",
    "scales = []\n",
    "rmse_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_x = None\n",
    "worst_y_true = None\n",
    "worst_y_pred = None\n",
    "worst_rmse = 0\n",
    "worst_shape, worst_scale = None, None\n",
    "worst_y_pdf = None\n",
    "\n",
    "for j in range(10000):\n",
    "    new_shape = np.random.random()*40.0\n",
    "    new_scale = np.random.random()*40.0\n",
    "    x = []\n",
    "    y = []\n",
    "    y_true = []\n",
    "\n",
    "    my_gamma = gamma(a=new_shape, scale=new_scale)    \n",
    "    x_range = np.arange(0.0, 500.0, 0.1) \n",
    "    pdf_range = my_gamma.pdf(x=x_range)\n",
    "    \n",
    "    y_true = my_gamma.cdf(x=x_range)\n",
    "    \n",
    "    \n",
    "    new_shape_range = [new_shape] * x_range.shape[0]\n",
    "    new_scale_range = [new_scale] * x_range.shape[0]\n",
    "    \n",
    "    x_input = torch.tensor(np.array([new_shape_range, new_scale_range, pdf_range, x_range], dtype=np.float32).T)\n",
    "    \n",
    "    net.eval()\n",
    "    y_pred = net(x_input).squeeze()\n",
    "    \n",
    "    \n",
    "    y_pred = np.array(y_pred.detach())\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "\n",
    "    my_rmse = np.nanmax(np.abs((y_true - y_pred)))\n",
    "    if (my_rmse > worst_rmse):\n",
    "        worst_x = x_range\n",
    "        worst_y_true = y_true\n",
    "        worst_y_pred = y_pred\n",
    "        worst_rmse = my_rmse\n",
    "        worst_shape, worst_scale = new_shape, new_scale\n",
    "        worst_y_pdf = pdf_range\n",
    "\n",
    "\n",
    "    scales.append(new_scale)\n",
    "    shapes.append(new_shape)\n",
    "    rmse_scores.append(my_rmse)\n",
    "    print(j, end=\"\\r\")\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(worst_x, worst_y_pred, label=\"Prediction\")\n",
    "plt.plot(worst_x, worst_y_true, label=\"True CDF\")\n",
    "plt.plot(worst_x, worst_y_pdf, label=\"True PDF\")\n",
    "plt.title(f\"shape={worst_shape}, scale={worst_scale}, RMSE={worst_rmse}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(scales, shapes, c=-np.array(rmse_scores), s=30, cmap = 'RdBu')\n",
    "plt.xlabel(\"Scale\")\n",
    "plt.ylabel(\"Shape\")\n",
    "print(np.nanmin(rmse_scores), \"-\",np.nanmax(rmse_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(worst_x, np.log(worst_y_true), label=\"True CDF\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
