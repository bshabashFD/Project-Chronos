{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Linear Regression Construction with Pyro\n",
    "\n",
    "One of the most beneficial uses for probabilistic programming is the construction of probabilistic machine learning models.\n",
    "\n",
    "Generally speaking, machine learning models are designed as functions, or mappings, from an input to an output. We have some input $X$ which is an $m\\times n$ matrix of $m$ data points with $n$ features each, and we want to construct some mapping $f$ which transforms the features into predictions of some unknown values $\\vec{y} = \\mathbb{R}^n$\n",
    "\n",
    "in essence, we're interested in a function:\n",
    "\n",
    "$$f(X) = y$$\n",
    "\n",
    "and we'd like our ML models to approximate it as best as possible, ideally while also giving us an idea for **why** the produced certain predictions.\n",
    "\n",
    "One of the basic ML models which accomplishes both is the linear regression which simply maps $y$ as a linear response of $X$ given some coefficients, $\\beta$, such that\n",
    "\n",
    "$$ y = \\beta X$$\n",
    "\n",
    "However, one major drawback of simple linear regressions is that they create point estimates of $\\beta$ without taking uncertainty into account. In this article we'll discuss how to leverage PyTorch and Pyro to produce linear regression models which create uncertainty estimates both for the parameters, as well as for predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employing Scikit-learn's Linear Regression\n",
    "\n",
    "We'll start by exploring a simple linear regression from `sklearn`, and see how it behaves on one of the built in datasets in `sklearn`, the California Housing dataset.\n",
    "\n",
    "We'll start by importing all our required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import the housing dataset, and explore its features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_start = time.time()\n",
    "super_results = pd.DataFrame({'LR' :[], \n",
    "                 'MCMC unscaled normal': [], \n",
    "                 'MCMC scaled normal': [], \n",
    "                 'MCMC gamma': [], \n",
    "                 'MCMC gamma+cen': [], \n",
    "                 'SVI gamma+cen': []})\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "california = fetch_california_housing()\n",
    "\n",
    "X = california.data\n",
    "y = california.target * 100000\n",
    "\n",
    "print(f'Data shape is {X.shape}')\n",
    "print(f'Target shape is {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.hist(y, bins=100, color=\"blue\", edgecolor=\"black\")\n",
    "plt.xlabel(\"Median House Value (in $100K)\", size=16)\n",
    "plt.ylabel(\"Frequency\", size=16)\n",
    "plt.title(\"A Histogram of California Median House Values Circa 1990\", size=22)\n",
    "plt.xticks(size=16)\n",
    "plt.yticks(size=16)\n",
    "plt.savefig(\"HouseValues.png\", dpi=36*4)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two important observations to be made from this data; First, the distribution of prices is not normal, it is closer to a Gamma distribution. Second, there is a very sharp spike of instances at the \\\\$500,000 mark, this can an indication of \"censored\" data. In essence, the data might have been recorded up to \\\\$500,000, at which point it was recorded as \"\\\\$500,000+\"\n",
    "\n",
    "Let's explore how well a linear regression performs on this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_threshold = 5.0 * 100000\n",
    "\n",
    "my_linear_regression = LinearRegression().fit(X_train,y_train)\n",
    "\n",
    "print(my_linear_regression.intercept_)\n",
    "print(my_linear_regression.coef_)\n",
    "\n",
    "y_pred = my_linear_regression.predict(X_test)\n",
    "\n",
    "super_results.loc['runtime', 'LR'] = 0.0\n",
    "\n",
    "y_small = y_test[y_test<max_threshold]\n",
    "y_pred_small = y_pred[y_test<max_threshold]\n",
    "\n",
    "y_pred = np.where(y_pred > max_threshold, max_threshold, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(y_pred, y_test, c=\"blue\", edgecolor=\"black\", label=f'R^2={round(r2_score(y_test, y_pred),2)}')\n",
    "plt.title(\"Predicted House Price vs Actual House Price\", size=22)\n",
    "plt.xlabel(\"Predicted House Price (in $100K)\", size=16)\n",
    "plt.ylabel(\"True House Price (in $100K)\", size=16)\n",
    "plt.ticklabel_format(style='plain')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_results.loc['R^2 censored', 'LR'] = r2_score(y_test, y_pred)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(y_pred_small, y_small, c=\"blue\", edgecolor=\"black\", label=f'R^2={round(r2_score(y_small, y_pred_small),2)}')\n",
    "plt.title(\"Predicted House Price vs Actual House Price\", size=22)\n",
    "plt.xlabel(\"Predicted House Price (in $100K)\", size=16)\n",
    "plt.ylabel(\"True House Price (in $100K)\", size=16)\n",
    "plt.ticklabel_format(style='plain')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_results.loc['R^2 small', 'LR'] = r2_score(y_small, y_pred_small)\n",
    "r2_score(y_small, y_pred_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this approach can produce satisfactory results, it suffers from a few main drawbacks; \n",
    "\n",
    "* First, the linear model generally ignore the fact that the prices come from a Gamma distribution. Its calculations of the expected value for every point are predicated on the mean coming from a Normal distribution. \n",
    "* Second, and more importantly, the simple linear model takes the censored data at face value. That is, it believes the prices of all houses registered as \"+\\\\$500,000\" to be \\\\$500,000. This might cause it to underestimate the effect some features have on house price.\n",
    "* Third, for each coefficient, we only get a point estimate of its most likely value (under certain incorrect assumptions). However, we might be interested in a range which accounts for uncertainty. For example, we might want to know what is the range of price increases we can expect for each additional bedroom.\n",
    "\n",
    "To address these problems, we can employ Pyro and PyTorch to construct our own linear model which will address all the pain points just mentioned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step I - Reconstructing the Linear Model with Pyro\n",
    "\n",
    "First, let's try and replicate the findings of the simple linear regression with Pyro. This will give us an intuition for how the different Pyro primitives work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required Pyro/Pytorch libraries\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "# We will use Markov Chain Monte Carlo (MCMC) methods here, specifically the No U-Turn Sampler (NUTS)\n",
    "from pyro.infer import MCMC, NUTS\n",
    "\n",
    "# Fix the random seeds\n",
    "#torch.manual_seed(0)\n",
    "#pyro.set_rng_seed(0)\n",
    "\n",
    "# We'll be timing our execution as well\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define our model in Pyro. Pyro models are defined as functions (actually they are defined as callables, but the simplest callable is a function). The function will accept our features $X$, our target $y$, and also the feature names for easier naming of priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_normal(X, y, column_names):\n",
    "    \n",
    "    # Define our intercept value\n",
    "    linear_combination = pyro.sample(f\"beta_intercept\", dist.Normal(0.0, 1.0))\n",
    "    \n",
    "    \n",
    "    betas = pyro.sample(f\"beta_coefficients\", dist.Normal(0.0, 1.0).expand([X.shape[1]]).to_event(1)).double()\n",
    "    \n",
    "    linear_combination = linear_combination + torch.matmul(X, betas)\n",
    "    \n",
    "    # Define a sigma value for the random error\n",
    "    sigma = pyro.sample(\"sigma\", dist.HalfCauchy(scale=10.0))\n",
    "    \n",
    "    # For a simple linear model, the expected mean is the linear combination of parameters\n",
    "    mean = linear_combination\n",
    "    \n",
    "    \n",
    "    with pyro.plate(\"data\", y.shape[0]):\n",
    "        \n",
    "        # Assume our expected mean comes from a normal distribution\n",
    "        outcome_dist = dist.Normal(mean, sigma)\n",
    "        \n",
    "        # Condition the expected mean on the observed target y\n",
    "        observation = pyro.sample(\"obs\", outcome_dist, obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_normal_old(X, y, column_names):\n",
    "    \n",
    "    # Define our intercept value\n",
    "    linear_combination = pyro.sample(f\"beta_intercept\", dist.Normal(0.0, 1.0))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define each coefficient using the column name\n",
    "    for i in range(0, X.shape[1]):\n",
    "        beta_i = pyro.sample(f\"beta_{column_names[i]}\", dist.Normal(0.0, 1.0))\n",
    "        linear_combination = linear_combination + (beta_i * X[:, i])\n",
    "    \n",
    "    # Define a sigma value for the random error\n",
    "    sigma = pyro.sample(\"sigma\", dist.HalfCauchy(scale=10.0))\n",
    "    \n",
    "    # For a simple linear model, the expected mean is the linear combination of parameters\n",
    "    mean = linear_combination\n",
    "    \n",
    "    \n",
    "    with pyro.plate(\"data\", y.shape[0]):\n",
    "        \n",
    "        # Assume our expected mean comes from a normal distribution\n",
    "        outcome_dist = dist.Normal(mean, sigma)\n",
    "        \n",
    "        # Condition the expected mean on the observed target y\n",
    "        observation = pyro.sample(\"obs\", outcome_dist, obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence what we've done here is define our linear regression as the following linear combination of parameters\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "However, unlike traditional linear regressions, we've defined each beta coefficient to be a distribution instead of a single values. That is, for each coefficient, we can ask what is the range of possible values this coefficient can assume given the data we observed. We gave a name to each of those distributions (e.g. \"`beta_intercept`\") for easy reference later.\n",
    "\n",
    "We had to define priors on each coefficient. A prior is like our \"best guess\" for that coefficient. Once the priors are defined, we can ask Pyro to update them into better and better guesses through the magic of MCMC samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_NUMBER = 3000\n",
    "\n",
    "\n",
    "# Turn out numpy data into PyTorch \n",
    "# tensors\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.float64)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float64)\n",
    "\n",
    "\n",
    "\n",
    "# Clear the parameter storage\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# Initialize our No U-Turn Sampler\n",
    "my_kernel = NUTS(model_normal, max_tree_depth=7)\n",
    "\n",
    "# Employ the sampler in an MCMC sampling \n",
    "# algorithm, and sample 3100 samples. \n",
    "# Then remove the first 100\n",
    "my_mcmc1 = MCMC(my_kernel,\n",
    "                num_samples=SAMPLE_NUMBER,\n",
    "                warmup_steps=100)\n",
    "\n",
    "\n",
    "# Let's time our execution as well\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the sampler\n",
    "my_mcmc1.run(X_train_torch, \n",
    "             y_train_torch, \n",
    "             california.feature_names)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Inference ran for {round(end_time -  start_time, 2)} seconds')\n",
    "\n",
    "super_results.loc['runtime', 'MCMC unscaled normal'] = round(end_time -  start_time, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not going to explain in detail how the sampling works here, but if you are interested in a breakdown of what has happened here, I recommend that you check out [my previous post](https://towardsdatascience.com/probabilistic-programming-with-pyro-and-kitchen-scale-f8d6a5d9ae0f) which explores the use of MCMC methods to optimize one parameter.\n",
    "\n",
    "Once our sampler is finished, we can look at the summary data for each beta value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mcmc1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the individual samples from our sampler, let's turn those into a dataframe (they are returned as a dictionary).\n",
    "We can grab the mean of each distribution as a coefficient point estimate, and then calculate a set of predictions for our data points. Then we can compare them to our known values for house prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_df = pd.DataFrame(my_mcmc1.get_samples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_df = pd.DataFrame(my_mcmc1.get_samples())\n",
    "beta_df['beta_coefficients'] = beta_df['beta_coefficients'].apply(lambda x: [i.detach().numpy() for i in x])\n",
    "\n",
    "\n",
    "df_columns = [f'beta_{i}' for i in california.feature_names]\n",
    "coefficient_df = pd.DataFrame(beta_df['beta_coefficients'].tolist(), \n",
    "                              index=beta_df['beta_coefficients'].index, columns=df_columns)\n",
    "beta_df = pd.concat([beta_df, coefficient_df], axis=1).drop('beta_coefficients', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "extra_col_name = beta_df.columns[1]\n",
    "extra_col_values = beta_df[extra_col_name]\n",
    "beta_df.drop(extra_col_name, axis=1, inplace=True)\n",
    "beta_df[extra_col_name] = extra_col_values\n",
    "beta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_linear_combination(beta_df, X):\n",
    "    \n",
    "    # Don't grab the last column, that is our estimate of the error standard deviation, \"sigma\"\n",
    "    coefficients = beta_df.iloc[:, :-1].mean()\n",
    "    coefficients\n",
    "\n",
    "    # Find our linear combination again\n",
    "    linear_combination = X.dot(coefficients[1:]) + coefficients.iloc[0]\n",
    "    \n",
    "    return linear_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_combination = predict_linear_combination(beta_df, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our predictions are the linear combination\n",
    "y_pred = linear_combination\n",
    "\n",
    "# Find all the predictions for houses below $500,000 in value\n",
    "y_pred_small = y_pred[y_test<max_threshold]\n",
    "\n",
    "# Censor our predictions as well\n",
    "y_pred= np.where(y_pred > max_threshold, max_threshold, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare our entire dataset. As well as our estimate of non-censored data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(y_pred, y_test, c=\"blue\", edgecolor=\"black\", label=f'R^2={round(r2_score(y_test, y_pred), 2)}')\n",
    "plt.title(\"Predicted House Price vs Actual House Price (Entire Dataset)\", size=22)\n",
    "plt.xlabel(\"Predicted House Price (in $100K)\", size=16)\n",
    "plt.ylabel(\"True House Price (in $100K)\", size=16)\n",
    "plt.ticklabel_format(style='plain')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_results.loc['R^2 censored', 'MCMC unscaled normal'] = r2_score(y_test, y_pred)\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(y_pred_small, y_small, c=\"blue\", edgecolor=\"black\", label=f'R^2={round(r2_score(y_small, y_pred_small),2)}')\n",
    "plt.title(\"Predicted House Price vs Actual House Price (Non-censored Data Points)\", size=22)\n",
    "plt.xlabel(\"Predicted House Price (in $100K)\", size=16)\n",
    "plt.ylabel(\"True House Price (in $100K)\", size=16)\n",
    "plt.ticklabel_format(style='plain')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_results.loc['R^2 small', 'MCMC unscaled normal'] = r2_score(y_small, y_pred_small)\n",
    "print(r2_score(y_small, y_pred_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that looks like a disaster! What happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that will draw the coefficients' distributions for us when given a coefficient dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_coefficients(beta_df):\n",
    "    figure, axis = plt.subplots(5,2, figsize=(15,15))\n",
    "\n",
    "\n",
    "    for key, ax in zip(beta_df.columns, axis.ravel()):\n",
    "        ax.set_title(key)\n",
    "        ax.hist(beta_df[key], color=\"blue\", edgecolor=\"black\", bins=100)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_coefficients(beta_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look interesting... they don't look anything like the coefficients the linear regression from `sklearn` has found. \n",
    "\n",
    "Turns out MCMC methods have a hard time with different scales for our data. They work much better when our features and target are scaled. Let's explore the performance of the same methodology with the data scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to scale our X and y data using a standard scaler from `sklearn`. The standard scaler will scale each feature in X in such a way that it has a mean of 0.0, and a standard deviation of 1.0.\n",
    "\n",
    "This means, instead of trying to find a set of coefficients on the original data\n",
    "\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n $$\n",
    "\n",
    "we will find a set of coefficients on the scaled data:\n",
    "\n",
    "$$y' = \\beta'_0 + \\beta'_1\\frac{x_1 - \\mu_1}{\\sigma_1} + \\beta'_2\\frac{x_2 - \\mu_2}{\\sigma_2} + ... + \\beta'_n\\frac{x_n - \\mu_n}{\\sigma_n} $$\n",
    "\n",
    "Where \n",
    "\n",
    "$$y' = \\frac{y}{max(y)}$$\n",
    "\n",
    "That is y is scaled to be between 0-1 and the columns of X have been standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_normal(X, y, column_names):\n",
    "    \n",
    "    # Define our intercept value\n",
    "    linear_combination = pyro.sample(f\"beta_intercept\", dist.Normal(0.0, 1.0))\n",
    "    \n",
    "\n",
    "    betas = pyro.sample(f\"beta_coefficients\", dist.Normal(0.0, 1.0).expand([X.shape[1]]).to_event(1)).double()\n",
    "    linear_combination = linear_combination + torch.matmul(X, betas)\n",
    "    \n",
    "    # Define a sigma value for the random error\n",
    "    sigma = pyro.sample(\"sigma\", dist.HalfCauchy(scale=10.0))\n",
    "    \n",
    "    # For a simple linear model, the expected mean is the linear combination of parameters\n",
    "    mean = linear_combination\n",
    "    \n",
    "    \n",
    "    with pyro.plate(\"data\", y.shape[0]):\n",
    "        \n",
    "        # Assume our expected mean comes from a normal distribution\n",
    "        outcome_dist = dist.Normal(mean, sigma)\n",
    "        \n",
    "        # Condition the expected mean on the observed target y\n",
    "        observation = pyro.sample(\"obs\", outcome_dist, obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "my_x_scaler = StandardScaler()\n",
    "X_train_scaled = my_x_scaler.fit_transform(X_train)\n",
    "X_test_scaled = my_x_scaler.transform(X_test)\n",
    "\n",
    "y_max = y_train.max()\n",
    "y_train_scaled = y_train/y_max\n",
    "y_test_scaled = y_test/y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float64)\n",
    "y_train_torch = torch.tensor(y_train_scaled, dtype=torch.float64)\n",
    "\n",
    "\n",
    "pyro.clear_param_store()\n",
    "my_kernel = NUTS(model_normal, max_tree_depth=7)\n",
    "\n",
    "\n",
    "my_mcmc2 = MCMC(my_kernel,\n",
    "                num_samples=SAMPLE_NUMBER,\n",
    "                warmup_steps=100)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "my_mcmc2.run(X_train_torch,\n",
    "             y_train_torch, \n",
    "             california.feature_names)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Inference ran for {round(end_time -  start_time, 2)} seconds')\n",
    "super_results.loc['runtime', 'MCMC scaled normal'] = round(end_time -  start_time, 2)\n",
    "\n",
    "\n",
    "\n",
    "my_mcmc2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithm ran much faster now, but if we recover the coefficients the algorithm found, those will be the coefficients on the scaled data. We would like to translate them back into the unscaled data so we can ask questions such as: \"For each extra bedroom in the house, what will be the effect on the price?\"\n",
    "\n",
    "\n",
    "Luckily, we can manipulate our equation to retrieve the coefficients on the unscaled data. We begin with our original equation\n",
    "\n",
    "$$y' = \\beta'_0 + \\beta'_1\\frac{x_1 - \\mu_1}{\\sigma_1} + \\beta'_2\\frac{x_2 - \\mu_2}{\\sigma_2} + ... + \\beta'_n\\frac{x_n - \\mu_n}{\\sigma_n} $$\n",
    "\n",
    "and we expand each fraction:\n",
    "\n",
    "$$y' = \\beta'_0 + \\beta'_1\\frac{x_1}{\\sigma_1} - \\beta'_1\\frac{\\mu_1}{\\sigma_1} + \\beta'_2\\frac{x_2}{\\sigma_2} - \\beta'_2\\frac{\\mu_2}{\\sigma_2} + ... + \\beta'_n\\frac{x_n}{\\sigma_n} - \\beta'_n\\frac{\\mu_n}{\\sigma_n} $$\n",
    "\n",
    "We can then rearrange the equation as follows:\n",
    "\n",
    "$$y' = \\beta'_0 - \\beta'_1\\frac{\\mu_1}{\\sigma_1} - \\beta'_2\\frac{\\mu_2}{\\sigma_2} ... - \\beta'_n\\frac{\\mu_n}{\\sigma_n} + \\frac{\\beta'_1}{\\sigma_1}x_1 + \\frac{\\beta'_2}{\\sigma_2}x_2 + ... + \\frac{\\beta'_n}{\\sigma_n}x_n  $$\n",
    "\n",
    "Recalling that \n",
    "\n",
    "$$y' = \\frac{y}{max(y)}$$\n",
    "\n",
    "we can finally rewrite our formula as follows:\n",
    "\n",
    "$$y = max(y)\\big( \\beta'_0 - \\sum_{i=1}^n\\beta'_i\\frac{\\mu_i}{\\sigma_i} \\big) + \\frac{\\beta'_1 max(y)}{\\sigma_1}x_1 + \\frac{\\beta'_2 max(y)}{\\sigma_2}x_2 + ... + \\frac{\\beta'_n max(y)}{\\sigma_n}x_n  $$\n",
    "\n",
    "We can create a function to perform the processing of the coefficients from the scaled data to the unscaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_beta_df(beta_df, x_scaler, feature_names):        \n",
    "        \n",
    "    beta_df['beta_coefficients'] = beta_df['beta_coefficients'].apply(lambda x: [i.detach().numpy() for i in x])\n",
    "\n",
    "\n",
    "    df_columns = [f'beta_{i}' for i in feature_names]\n",
    "    coefficient_df = pd.DataFrame(beta_df['beta_coefficients'].tolist(), \n",
    "                                  index=beta_df['beta_coefficients'].index, columns=df_columns)\n",
    "    beta_df = pd.concat([beta_df, coefficient_df], axis=1).drop('beta_coefficients', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    extra_col_name = beta_df.columns[1]\n",
    "    extra_col_values = beta_df[extra_col_name]\n",
    "    beta_df.drop(extra_col_name, axis=1, inplace=True)\n",
    "    beta_df[extra_col_name] = extra_col_values\n",
    "    beta_df    \n",
    "        \n",
    "    \n",
    "    i = 0\n",
    "    for col in beta_df:\n",
    "        if (col != 'beta_intercept'):\n",
    "            if ('beta_' in col):\n",
    "\n",
    "                beta_df['beta_intercept'] -= (beta_df[col] * x_scaler.mean_[i])/x_scaler.scale_[i]\n",
    "                \n",
    "                \n",
    "                beta_df[col] /= x_scaler.scale_[i]\n",
    "\n",
    "                i += 1\n",
    "\n",
    "    return beta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta2_df = pd.DataFrame(my_mcmc2.get_samples())\n",
    "beta2_df = create_beta_df(beta2_df, my_x_scaler, california.feature_names)\n",
    "beta2_df *= y_max   # multiply everything by y_max outside the function for reasons that will\n",
    "                    # become clear later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These distributions seem much closer to the values found by the linear regression from `sklearn`. Let's compare the prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_combination = predict_linear_combination(beta2_df, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linear_combination\n",
    "y_pred_small = y_pred[y_test<max_threshold]\n",
    "y_pred= np.where(y_pred > max_threshold, max_threshold, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we will compare the results on the entire dataset, as well as on the uncensored labels only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(y_pred, y_test, c=\"blue\", edgecolor=\"black\", label=f'R^2={round(r2_score(y_test, y_pred), 2)}')\n",
    "plt.title(\"Predicted House Price vs Actual House Price (Entire Dataset)\", size=22)\n",
    "plt.xlabel(\"Predicted House Price (in $100K)\", size=16)\n",
    "plt.ylabel(\"True House Price (in $100K)\", size=16)\n",
    "plt.ticklabel_format(style='plain')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_results.loc['R^2 censored', 'MCMC scaled normal'] = r2_score(y_test, y_pred)\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(y_pred_small, y_small, c=\"blue\", edgecolor=\"black\", label=f'R^2={round(r2_score(y_small, y_pred_small),2)}')\n",
    "plt.title(\"Predicted House Price vs Actual House Price (Non-censored Data Points)\", size=22)\n",
    "plt.xlabel(\"Predicted House Price (in $100K)\", size=16)\n",
    "plt.ylabel(\"True House Price (in $100K)\", size=16)\n",
    "plt.ticklabel_format(style='plain')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_results.loc['R^2 small', 'MCMC scaled normal'] = r2_score(y_small, y_pred_small)\n",
    "print(r2_score(y_small, y_pred_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_coefficients(beta2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like we got a comparable performance.\n",
    "\n",
    "However, we can actually employ Pyro to do better! Recall that we saw our house prices are not normally distributed but in fact follow a Gamma distribution. We can modify our code to reflect that in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gamma Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to better reflect the house distribution, we can employ a Gamma distribution for our target values. Unlike the Normal distribution which is defined by its mean and standard deviation, the Gamma distribution is defined by two positive parameters which are the shape and the rate.\n",
    "\n",
    "When constructing our model for a distribution other than normal, we need to employ a **link function** which will translate the linear combination of our parameters to the expected value, or the mean, of the distribution. We also would like to know the relationship between the mean and the distribution parameters. Luckily, for the Gamma distribution this is predefined as:\n",
    "\n",
    "$$\\mu = \\frac{shape}{rate}$$\n",
    "\n",
    "However, if both the shape and rate parameters are positive, that means the mean must be positive as well. We need to make sure that our link function captures that. Therefore, I will use the following link function for the linear equation:\n",
    "\n",
    "$$ln(\\mu) = ln(y') = \\beta'_0 + \\beta'_1\\frac{x_1 - \\mu_1}{\\sigma_1} + \\beta'_2\\frac{x_2 - \\mu_2}{\\sigma_2} + ... + \\beta'_n\\frac{x_n - \\mu_n}{\\sigma_n}$$\n",
    "\n",
    "or \n",
    "\n",
    "$$y' = e^{\\beta'_0 + \\beta'_1\\frac{x_1 - \\mu_1}{\\sigma_1} + \\beta'_2\\frac{x_2 - \\mu_2}{\\sigma_2} + ... + \\beta'_n\\frac{x_n - \\mu_n}{\\sigma_n}}$$.\n",
    "\n",
    "\n",
    "Interestingly enough, to recover the coefficients for the unscaled data, the math works out fairly similarly except for the constant. Keeping in mind that:\n",
    "\n",
    "$$y' = \\frac{y}{max(y)}$$\n",
    "\n",
    "and that\n",
    "\n",
    "$$ln(\\frac{y}{max(y)}) = ln(y) - ln(max(y))$$\n",
    "\n",
    "We can find that our equation can be written as:\n",
    "\n",
    "$$ln(y) = \\big(ln(max(y)) + \\beta'_0 - \\sum_{i=1}^n\\beta'_i\\frac{\\mu_i}{\\sigma_i} \\big) + \\frac{\\beta'_1}{\\sigma_1}x_1 + \\frac{\\beta'_2}{\\sigma_2}x_2 + ... + \\frac{\\beta'_n}{\\sigma_n}x_n  $$\n",
    "\n",
    "or \n",
    "\n",
    "$$y = e^{\\big(ln(max(y)) + \\beta'_0 - \\sum_{i=1}^n\\beta'_i\\frac{\\mu_i}{\\sigma_i} \\big) + \\frac{\\beta'_1}{\\sigma_1}x_1 + \\frac{\\beta'_2}{\\sigma_2}x_2 + ... + \\frac{\\beta'_n}{\\sigma_n}x_n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_gamma(X, y, column_names):\n",
    "    \n",
    "    # We still need to calculate our linear combination\n",
    "    linear_combination = pyro.sample(f\"beta_intercept\", dist.Normal(0.0, 1.0))\n",
    "    \n",
    "    \n",
    "    betas = pyro.sample(f\"beta_coefficients\", dist.Normal(0.0, 1.0).expand([X.shape[1]]).to_event(1)).double()\n",
    "    linear_combination = linear_combination + torch.matmul(X, betas)\n",
    "    \n",
    "    # But now our mean will be e^{linear combination}\n",
    "    mean = torch.exp(linear_combination)\n",
    "    \n",
    "    # We will also define a rate parameter\n",
    "    rate = pyro.sample(\"rate\", dist.HalfCauchy(scale=10.0))\n",
    "    \n",
    "    # Since mean = shape/rate, then the shape = mean * rate\n",
    "    shape = mean * rate\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Now that we have the shape and rate parameters for the\n",
    "    # Gamma distribution, we can draw samples from it and condition\n",
    "    # them on our observations\n",
    "    with pyro.plate(\"data\", y.shape[0]):\n",
    "        \n",
    "        outcome_dist = dist.Gamma(shape, rate)\n",
    "        \n",
    "        observation = pyro.sample(\"obs\", outcome_dist, obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice our code is slightly different but we are still calculating a linear combination of our X data and our coefficients, except now we take the `exp` of that combination to get the expected value of our data point. \n",
    "We also sample a rate parameter, and use the mean and rate to calculate the appropriate shape parameter.\n",
    "\n",
    "Given our shape and rate parameters, we can define a Gamma distribution and ask Pyro to optimize the values this distribution depends on (namely, our coefficients and rate parameters) in order to build a model most likely based on our data.\n",
    "\n",
    "Let's optimize this new model and look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "my_x_scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = my_x_scaler.fit_transform(X_train)\n",
    "X_test_scaled = my_x_scaler.transform(X_test)\n",
    "\n",
    "y_max = y_train.max()\n",
    "y_train_scaled = y_train/y_max\n",
    "y_test_scaled = y_test/y_max\n",
    "\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float64)\n",
    "y_train_torch = torch.tensor(y_train_scaled, dtype=torch.float64)\n",
    "\n",
    "\n",
    "pyro.clear_param_store()\n",
    "my_kernel = NUTS(model_gamma, max_tree_depth=7)\n",
    "\n",
    "\n",
    "my_mcmc3 = MCMC(my_kernel,\n",
    "                num_samples=SAMPLE_NUMBER,\n",
    "                warmup_steps=100)\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "my_mcmc3.run(X_train_torch,\n",
    "             y_train_torch, \n",
    "             california.feature_names)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Inference ran for {round(end_time -  start_time, 2)} seconds')\n",
    "super_results.loc['runtime', 'MCMC gamma'] = round(end_time -  start_time, 2)\n",
    "\n",
    "\n",
    "\n",
    "my_mcmc3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta3_df = pd.DataFrame(my_mcmc3.get_samples())\n",
    "beta3_df = create_beta_df(beta3_df, my_x_scaler, california.feature_names)\n",
    "\n",
    "# Notice now we have to use the y_max value slightly differently\n",
    "beta3_df['beta_intercept'] += np.log(y_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this models performance by calculating the predictions and comparing them to the observed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_combination = predict_linear_combination(beta3_df, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to take the EXP of the linear combination now to get the expected values\n",
    "y_pred = np.exp(linear_combination)\n",
    "\n",
    "# Again, compare for both the censored and uncensored datasets\n",
    "y_pred_small = y_pred[y_test<max_threshold]\n",
    "y_pred= np.where(y_pred > max_threshold, max_threshold, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(y_pred, y_test, c=\"blue\", edgecolor=\"black\", label=f'R^2={round(r2_score(y_test, y_pred), 2)}')\n",
    "plt.title(\"Predicted House Price vs Actual House Price\", size=22)\n",
    "plt.xlabel(\"Predicted House Price (in $100K)\", size=16)\n",
    "plt.ylabel(\"True House Price (in $100K)\", size=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_results.loc['R^2 censored', 'MCMC gamma'] = r2_score(y_test, y_pred)\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(y_pred_small, y_small, c=\"blue\", edgecolor=\"black\", label=f'R^2={round(r2_score(y_small, y_pred_small),2)}')\n",
    "plt.title(\"Predicted House Price vs Actual House Price\", size=22)\n",
    "plt.xlabel(\"Predicted House Price (in $100K)\", size=16)\n",
    "plt.ylabel(\"True House Price (in $100K)\", size=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_results.loc['R^2 small', 'MCMC gamma'] = r2_score(y_small, y_pred_small)\n",
    "print(r2_score(y_small, y_pred_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_coefficients(beta3_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice a few interesting differences between the qualitative conclusions this model gives us as opposed to the Normal distribution model:\n",
    "* First, under the Normal distribution assumption, it seemed that the house price decreases for each additional room, but under the Gamma distribution assumption, the price seems to increase.\n",
    "* Second, under the Normal distribution assumption, the price seems to increase for each additional bedroom of the house, but under the Gamma distribution assumption, that conclusion does not seem as certain anymore. \n",
    "\n",
    "Other parameters seem to qualitatively agree between the two models. However, we want to be certain we correctly identified the quantitative values of the parameters, and we need to account for the censored data to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gamma + Censored data\n",
    "\n",
    "For any given house, we assume that its price comes from a Gamma distribution. However, we've observed in our data that if a house price is above \\\\$500,000, that price will be censored. The question becomes, how do we quantify this in our model.\n",
    "\n",
    "We can ask ourselves, \"For a given house, what is the probability that its price will be censored?\"\n",
    "\n",
    "The answer is that the probability is the same as the probability that the house price will be above \\\\$500,000. Well we have a clear way to ask a distribution for the probability a value will be below some threshold, it is the Cumulative Distribution Function (CDF) at that threshold. So 1 - CDF(500,000) will give us the probability that a house value will be above \\\\$500,000.\n",
    "\n",
    "We will use that information and add that to the model. We'll say that if the model sees a value below \\\\$500,000, that is the observed value of the house. But if the model sees a value of \\\\$500,000, the probability of observing that censoring is the result of a Bernoulli trial, with the probability of censoring \"success\" being 1-CDF(500,000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gamma as scipygamma\n",
    "def model_gamma_cen(X, y, column_names, censored_label):\n",
    "    pyro.enable_validation(True) \n",
    "    \n",
    "    # Again find the linear combination\n",
    "    linear_combination = pyro.sample(f\"beta_intercept\", dist.Normal(0.0, 1.0))\n",
    "    \n",
    "    \n",
    "    #for i in range(0, X.shape[1]):\n",
    "    #    beta_i = pyro.sample(f\"beta_{column_names[i]}\", dist.Normal(0.0, 1.0))\n",
    "    #    linear_combination = linear_combination + (beta_i * X[:, i])\n",
    "    betas = pyro.sample(f\"beta_coefficients\", dist.Normal(0.0, 1.0).expand([X.shape[1]]).to_event(1)).double()\n",
    "    linear_combination = linear_combination + torch.matmul(X, betas)\n",
    "    \n",
    "    # The mean is e^{linear combination}\n",
    "    mean = torch.exp(linear_combination)\n",
    "    mean = torch.max(mean, torch.tensor(torch.finfo(mean.dtype).eps).double()) # sometimes the linear\n",
    "                                                                         # combination is too small so\n",
    "                                                                         # e^ of that becomes 0\n",
    "                                                                         # We replace it with the min value\n",
    "                                                                         # torch can have in float32 mode\n",
    "    \n",
    "    # Sample a rate as well\n",
    "    rate = pyro.sample(\"rate\", dist.HalfCauchy(scale=10.0)).clamp(min=torch.finfo(X.dtype).eps)\n",
    "    #rate = torch.max(rate, torch.tensor(torch.finfo(rate.dtype).eps).double())\n",
    "    \n",
    "    # Calculate the shape based on the mean and rate\n",
    "    shape = mean * rate\n",
    "    \n",
    "    \n",
    "    \n",
    "    # The data is now divided into two outcomes: censored, and non-censored\n",
    "    with pyro.plate(\"data\", y.shape[0]):\n",
    "        \n",
    "        \n",
    "        #print(mean.min(), mean.max())\n",
    "        #print(linear_combination.min())\n",
    "        \n",
    "        #print(rate.min(), rate.max())\n",
    "        #print(shape.min(), shape.max())\n",
    "        \n",
    "        outcome_dist = dist.Gamma(shape, rate)\n",
    "        \n",
    "        \n",
    "        # If it is not censored, our observations are the directly observed\n",
    "        # values of y\n",
    "        with pyro.poutine.mask(mask = (censored_label == 0.0)):\n",
    "            observation = pyro.sample(\"obs\", outcome_dist, obs=y)\n",
    "            \n",
    "        # If it is censored, our observations are the observed\n",
    "        # censoring lables, and those come from\n",
    "        # a Bernoulli distribution where the probability\n",
    "        # of censoring is defined by the CDF of the Gamma distribution\n",
    "        with pyro.poutine.mask(mask = (censored_label == 1.0)):\n",
    "\n",
    "            truncation_prob = torch.tensor(1 - scipygamma(shape.detach(), rate.detach()).cdf(y).astype(np.float32))\n",
    "            #print(truncation_prob.min(), truncation_prob.max())\n",
    "            \n",
    "            censored_observation = pyro.sample(\"censorship\",\n",
    "                                               dist.Bernoulli(truncation_prob),\n",
    "                                               obs=torch.tensor(1.0))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice since `pyro` doesn't yet implement a CDF method for its Gamma distribution I had to use the Gamma distribution from `scipy`.\n",
    "\n",
    "Now we can run our model again, but this time sending both the observed prices and observed censoring of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "X_train_scaled = my_x_scaler.fit_transform(X_train)\n",
    "X_test_scaled = my_x_scaler.transform(X_test)\n",
    "\n",
    "y_max = y_train.max()\n",
    "y_train_scaled = y_train/y_max\n",
    "y_test_scaled = y_test/y_max\n",
    "\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float64)\n",
    "y_train_torch = torch.tensor(y_train_scaled, dtype=torch.float64)\n",
    "\n",
    "# Find censored labels\n",
    "numpy_censored_label = np.where(y_train >= y_max, 1.0, 0.0)\n",
    "censored_label = torch.tensor(numpy_censored_label, dtype=torch.float64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pyro.clear_param_store()\n",
    "my_kernel = NUTS(model_gamma_cen, max_tree_depth=7)\n",
    "\n",
    "\n",
    "my_mcmc4 = MCMC(my_kernel,\n",
    "                num_samples=SAMPLE_NUMBER,\n",
    "                warmup_steps=100)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "my_mcmc4.run(X_train_torch,\n",
    "             y_train_torch, \n",
    "             california.feature_names, \n",
    "             censored_label)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Inference ran for {round(end_time -  start_time, 2)} seconds')\n",
    "\n",
    "super_results.loc['runtime', 'MCMC gamma+cen'] = round(end_time -  start_time, 2)\n",
    "\n",
    "\n",
    "\n",
    "my_mcmc4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta4_df = pd.DataFrame(my_mcmc4.get_samples())\n",
    "beta4_df = create_beta_df(beta4_df, my_x_scaler, california.feature_names)\n",
    "\n",
    "beta4_df['beta_intercept'] += np.log(y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_combination = predict_linear_combination(beta4_df, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.exp(linear_combination)\n",
    "y_pred_small = y_pred[y_test<max_threshold]\n",
    "y_pred= np.where(y_pred > max_threshold, max_threshold, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(y_pred, y_test, c=\"blue\", edgecolor=\"black\", label=f'R^2={round(r2_score(y_test, y_pred), 2)}')\n",
    "plt.title(\"Predicted House Price vs Actual House Price\", size=22)\n",
    "plt.xlabel(\"Predicted House Price (in $100K)\", size=16)\n",
    "plt.ylabel(\"True House Price (in $100K)\", size=16)\n",
    "plt.ticklabel_format(style='plain')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_results.loc['R^2 censored', 'MCMC gamma+cen'] = r2_score(y_test, y_pred)\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(y_pred_small, y_small, c=\"blue\", edgecolor=\"black\", label=f'R^2={round(r2_score(y_small, y_pred_small),2)}')\n",
    "plt.title(\"Predicted House Price vs Actual House Price\", size=22)\n",
    "plt.xlabel(\"Predicted House Price (in $100K)\", size=16)\n",
    "plt.ylabel(\"True House Price (in $100K)\", size=16)\n",
    "plt.ticklabel_format(style='plain')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_results.loc['R^2 small', 'MCMC gamma+cen'] = r2_score(y_small, y_pred_small)\n",
    "print(r2_score(y_small, y_pred_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_coefficients(beta4_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import constraints\n",
    "def guide_gamma_cen(X, y, column_names, censored_label):\n",
    "    pyro.enable_validation(True) \n",
    "    \n",
    "    \n",
    "    mu_intercept = pyro.param(\"mu_intercept\", \n",
    "                              torch.tensor(0.0, dtype=torch.float64))\n",
    "    sigma_intercept = pyro.param(\"sigma_intercept\", \n",
    "                                 torch.tensor(1.0, dtype=torch.float64), \n",
    "                                 constraint=constraints.positive)\n",
    "\n",
    "    # Again find the linear combination\n",
    "    linear_combination = pyro.sample(f\"beta_intercept\", \n",
    "                                     dist.Normal(mu_intercept, sigma_intercept))\n",
    "    \n",
    "    \n",
    "    mu_coef = pyro.param(f\"mu_betas\", \n",
    "                         torch.zeros(X.shape[1], dtype=torch.float64))\n",
    "    sigma_coef = pyro.param(f\"sigma_betas\", \n",
    "                            torch.ones(X.shape[1], dtype=torch.float64), \n",
    "                            constraint=constraints.positive)\n",
    "        \n",
    "    betas = pyro.sample(f\"beta_coefficients\", dist.Normal(mu_coef, sigma_coef).expand([X.shape[1]]).to_event(1)).double()\n",
    "    \n",
    "\n",
    "    # Let's try and fit a Gamma distribution on the rate parameter\n",
    "    \n",
    "    shape_r = pyro.param(\"shape_r\", torch.tensor(10.0, dtype=torch.float64), constraint=constraints.positive)\n",
    "    rate_r = pyro.param(\"rate_r\", torch.tensor(10.0, dtype=torch.float64), constraint=constraints.positive)\n",
    "\n",
    "    \n",
    "    rate = pyro.sample(\"rate\", dist.Gamma(shape_r, rate_r))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import SVI, Trace_ELBO, TraceGraph_ELBO\n",
    "from pyro.infer import Predictive\n",
    "from pyro.optim import ClippedAdam\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal, AutoMultivariateNormal\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = my_x_scaler.fit_transform(X_train)\n",
    "X_test_scaled = my_x_scaler.transform(X_test)\n",
    "\n",
    "y_max = y_train.max()\n",
    "y_train_scaled = y_train/y_max\n",
    "y_test_scaled = y_test/y_max\n",
    "\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float64)\n",
    "y_train_torch = torch.tensor(y_train_scaled, dtype=torch.float64)\n",
    "\n",
    "# Find censored labels\n",
    "numpy_censored_label = np.where(y_train >= y_max, 1.0, 0.0)\n",
    "censored_label = torch.tensor(numpy_censored_label, dtype=torch.float64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pyro.clear_param_store()\n",
    "my_guide = guide_gamma_cen\n",
    "#my_guide = AutoDiagonalNormal(model_gamma_cen)\n",
    "\n",
    "\n",
    "\n",
    "my_svi = SVI(model=model_gamma_cen,\n",
    "             guide= my_guide,\n",
    "             optim=ClippedAdam({\"lr\": 0.01, 'clip_norm': 1.0}),\n",
    "             loss=TraceGraph_ELBO())\n",
    "\n",
    "losses = []\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(10000):\n",
    "\n",
    "    loss = my_svi.step(X_train_torch,\n",
    "                       y_train_torch, \n",
    "                       california.feature_names, \n",
    "                       censored_label)\n",
    "    \n",
    "    normalized_loss = loss/X_train_torch.shape[0]\n",
    "    losses.append(normalized_loss)\n",
    "    if (i % 1000 == 0):\n",
    "        print(f'iter: {i}, normalized loss:{round(normalized_loss,2)}')\n",
    "        \n",
    "        \n",
    "\n",
    "predictive = Predictive(model=model_gamma_cen,\n",
    "                        guide= my_guide,\n",
    "                        num_samples=SAMPLE_NUMBER) \n",
    "\n",
    "samples = predictive(X_train_torch,\n",
    "                     y_train_torch, \n",
    "                     california.feature_names, \n",
    "                     censored_label)\n",
    "\n",
    "        \n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Inference ran for {round(end_time -  start_time, 2)} seconds')\n",
    "\n",
    "super_results.loc['runtime', 'SVI gamma+cen'] = round(end_time -  start_time, 2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_svi_df = {}\n",
    "for key, values in samples.items():\n",
    "    if (\"beta_\" in key) or (\"rate\" in key):\n",
    "        if (\"beta_coefficients\" in key):\n",
    "            values = values.view(values.shape[0], values.shape[-1])\n",
    "        else:\n",
    "            values = values.view(values.shape[0], )\n",
    "        \n",
    "        beta_svi_df[key] = values.detach()\n",
    "        \n",
    "beta_svi_df = pd.DataFrame(beta_svi_df)\n",
    "\n",
    "losses = np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(np.log(losses+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_svi_df = create_beta_df(beta_svi_df, my_x_scaler, california.feature_names)\n",
    "beta_svi_df['beta_intercept'] += np.log(y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_combination = predict_linear_combination(beta4_df, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.exp(linear_combination)\n",
    "y_pred_small = y_pred[y_test<max_threshold]\n",
    "y_pred= np.where(y_pred > max_threshold, max_threshold, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(y_pred, y_test, c=\"blue\", edgecolor=\"black\", label=f'R^2={round(r2_score(y_test, y_pred), 2)}')\n",
    "plt.title(\"Predicted House Price vs Actual House Price\", size=22)\n",
    "plt.xlabel(\"Predicted House Price (in $100K)\", size=16)\n",
    "plt.ylabel(\"True House Price (in $100K)\", size=16)\n",
    "plt.ticklabel_format(style='plain')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_results.loc['R^2 censored', 'SVI gamma+cen'] = r2_score(y_test, y_pred)\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(y_pred_small, y_small, c=\"blue\", edgecolor=\"black\", label=f'R^2={round(r2_score(y_small, y_pred_small),2)}')\n",
    "plt.title(\"Predicted House Price vs Actual House Price\", size=22)\n",
    "plt.xlabel(\"Predicted House Price (in $100K)\", size=16)\n",
    "plt.ylabel(\"True House Price (in $100K)\", size=16)\n",
    "plt.legend()\n",
    "plt.ticklabel_format(style='plain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_results.loc['R^2 small', 'SVI gamma+cen'] = r2_score(y_small, y_pred_small)\n",
    "print(r2_score(y_small, y_pred_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_coefficients(beta_svi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_end = time.time()\n",
    "\n",
    "print(f'Notebook runtime: {round((super_end -  super_start)/60, 2)} minutes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
